{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotify Random Forest Classifer\n",
    "\n",
    "For this project, I was interested in making use of my personal Spotify streaming data I requested directly from Spotify a few months ago. Any user can do this by going to the Spotify website, logging in, and then there is a page where you can submit a request for your own data. The insights I was interested in with this experiment had to do with predicting and understanding different features, such as artist, track title, and reason a song was started, and how they impact my likelihood to complete a song. To do this, I build a random forest classifier that uses a target of \"trackdone\", which is the Spotify reason code for a song ending in the data, and created a binary classification that stated whether the song was \"completed\" or \"not completed\". In the following code, you will see my EDA process to wrangle and clean the data to create a proper dataset for this model. You will also see the functionality of my model, and then ultimately the results as well as some visuals. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first block, I take all the json files that Spotify sends the data in and create an index for each file to be extended to. I then take the keys of these json dictionaries containing each record, and turn them into the headers for my CSV. Finally, I write the value in as records for each header to create my CSV file to be imported into the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "\n",
    "#path to spotify json files\n",
    "json_files = glob.glob('/Users/mason/Documents/Data Science/spotify audio json files/*.json')\n",
    "\n",
    "#initialize index for all data from path\n",
    "all_spotify_data = []\n",
    "\n",
    "#read all json files in path\n",
    "for file in json_files:\n",
    "    with open(file, 'r', encoding = 'utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        all_spotify_data.extend(data)\n",
    "\n",
    "#get unique keys\n",
    "keys = set()\n",
    "for entry in all_spotify_data:\n",
    "    keys.update(entry.keys())\n",
    "keys = list(keys)\n",
    "\n",
    "#write data to csv\n",
    "with open('ms_spotify_streaming_histor_cleaned.csv','w', newline='', encoding = 'utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_spotify_data)\n",
    "   \n",
    "\n",
    "print(\"CSV File Created Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block takes the CSV created from the script above. I then explore the dataset, remove any unnecessary columns, NA values, convert the timestamp to date time, and then do some reordering to make the dataset look how I want it to. Initially I was going to run this experiment on all the data in my streaming history available, but there was around 8 years of data containing over 200k records, which really slowed down the model run time. For the sake of this project, I chose to only explore data starting from the beginning of January, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#bring in CSV created from import json script\n",
    "df = pd.read_csv('/Users/mason/Documents/Data Science/spotify audio json files/ms_spotify_streaming_history.csv')\n",
    "\n",
    "#drop initial unnecessary columns\n",
    "df = df.drop(columns=['offline','shuffle','audiobook_chapter_title','spotify_track_uri','incognito_mode',\n",
    "                       'audiobook_uri','episode_show_name','audiobook_chapter_uri', 'conn_country',\n",
    "                       'platform','episode_name','spotify_episode_uri','audiobook_title','ip_addr',\n",
    "                       'offline_timestamp'])\n",
    "\n",
    "\n",
    "#convert the timestamp to datetime               \n",
    "df['ts'] = pd.to_datetime(df['ts'])\n",
    "df['date'] = df['ts'].dt.date\n",
    "\n",
    "df['seconds played'] = (df['ms_played'] / 1000).round()\n",
    "\n",
    "#reorder the fields for visual purposes\n",
    "df = df.reindex(columns=['date','ts','master_metadata_album_artist_name','master_metadata_track_name',\n",
    "                        'master_metadata_album_album_name','reason_start','reason_end','skipped',\n",
    "                        'seconds played','ms_played'])\n",
    "\n",
    "\n",
    "#drop the original timestamp column now that the date is extracted\n",
    "df= df.drop(columns=['ts','ms_played'])\n",
    "\n",
    "\n",
    "#drop NA values from fields containing NAs (due to removed fields such as audiobook titles)\n",
    "#for this analysis, I am only focused on music\n",
    "df = df.dropna(subset=['master_metadata_album_artist_name','reason_start','reason_end'])\n",
    "\n",
    "print(df.isna().sum())\n",
    "\n",
    "#rename my fields to my liking\n",
    "df = df.rename(columns={'master_metadata_album_artist_name': 'artist', \n",
    "                        'master_metadata_track_name': 'track title',\n",
    "                        'master_metadata_album_album_name': 'album title'})\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "start_date = pd.to_datetime('2023-01-01')\n",
    "end_date = pd.to_datetime('2025-01-08')\n",
    "\n",
    "\n",
    "filtered_df = df.loc[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "\n",
    "#export to csv so I can view in excel if desired\n",
    "df.to_csv('cleaned_data.csv',index=False)\n",
    "\n",
    "filtered_df.to_csv('cleaned_data_01012023_01082025.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my clean dataset, I can build my model in the following blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all libraries I will be using for my model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset for model\n",
    "def load_data(filepath, nrows=None):\n",
    "    df = pd.read_csv(filepath, nrows=nrows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = load_data('/Users/mason/Documents/Data Science/spotify audio json files/cleaned_data_01012023_01082025.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates the target variable for the experiment. In this case it is marking a record as completed\n",
    "#or not completed based on whether the reason end is \"trackdone\".\n",
    "def create_target(df, target_col='completed', target_func=None):\n",
    "    if target_func is not None:\n",
    "        df[target_col] = df.apply(target_func, axis=1)\n",
    "    else:\n",
    "        df[target_col] = (df['reason_end'] == 'trackdone').astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target\n",
    "df = create_target(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this function, I encoded the features I would be using since they are categorical by assigning dummies to make them numerical.\n",
    "def prepare_features(df, features, categorical_features):\n",
    "    X = df[features]\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_features)\n",
    "    return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features can be modified for different experiments. *note: more features create a better model\n",
    "# Prepare features\n",
    "features = ['artist', 'track title', 'reason_start', 'album title']\n",
    "categorical_features = ['artist', 'track title', 'reason_start', 'album title']\n",
    "X_encoded = prepare_features(df, features, categorical_features)\n",
    "y = df['completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this function, I create my train/test split for the model \n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training\n",
    "X_train, X_test, y_train, y_test = split_data(X_encoded, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this function, I train the RF classifier and find the best hyperparameters for the model using grid search and cross validation.\n",
    "def train_rf(X_train, y_train, param_grid=None):\n",
    "    if param_grid is None:\n",
    "        param_grid = {'max_depth': [5, 10, 20, None],'max_leaf_nodes': [10, 20, 50, 100, None]}\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validated ROC-AUC:\", grid_search.best_score_)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train model using random forest classifier\n",
    "best_rf = train_rf(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I added this function after initial experimentation to create a better balance between recall and precision. Originally, my recall\n",
    "#was very high, but precision was lower, so I wanted to build a help to choose the best probability to maximize precision while still keeping\n",
    "#recall above a 0.7 at least.\n",
    "def find_best_threshold_for_precision(y_test, y_proba, target_recall=0.7):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "    # Find all thresholds where recall >= target_recall\n",
    "    possible = [(p, r, t) for p, r, t in zip(precisions, recalls, np.append(thresholds, 1.0)) if r >= target_recall]\n",
    "    if possible:\n",
    "        best = max(possible, key=lambda x: x[0])  # maximize precision\n",
    "        best_precision, best_recall, best_threshold = best\n",
    "        print(f\"\\nBest threshold for precision with recall >= {target_recall}: {best_threshold:.2f}\")\n",
    "        print(f\"Precision: {best_precision:.3f}, Recall: {best_recall:.3f}\")\n",
    "    else:\n",
    "        best_threshold = 0.5\n",
    "        print(\"\\nNo threshold found with recall above target. Using 0.5.\")\n",
    "    return best_threshold, precisions, recalls, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Predict probabilities\n",
    "y_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#  Find best threshold for precision with recall >= target. Target recall can be adjusted to fine tune the model\n",
    "target_recall = 0.7 \n",
    "best_threshold, precisions, recalls, thresholds = find_best_threshold_for_precision(y_test, y_proba, target_recall=target_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this function, I evaluate my model by returning my metrics for accuracy, precision, and recall at the best threshold. \n",
    "#I also added in a few visuals for additional insight as well as a list of the top 10 most imprtant features in my dataset that impact the models\n",
    "#ability to predict whether a song will be completed or not. *Note: importance in the top 10 doesnt mean it is likely to impact \n",
    "#the positive class, it means it is just an important factor when classifying. ex: Knox is one of my favorite artists which makes sense\n",
    "#why it is on there, but so is Taylor Swift, which also makes sense as I know I am almost guaranteed to hit skip for her music when it comes on.\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, best_threshold, precisions, recalls, thresholds):\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "    print(\"\\nTest Set Metrics at chosen threshold:\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Completed', 'Completed'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = X_test.columns\n",
    "    feat_imp = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    feat_imp = feat_imp.sort_values('importance', ascending=False)\n",
    "    print(\"\\nTop 10 Feature Importances:\")\n",
    "    print(feat_imp.head(10))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feat_imp['feature'][:10][::-1], feat_imp['importance'][:10][::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(thresholds, precisions[:-1], label='Precision')\n",
    "    plt.plot(thresholds, recalls[:-1], label='Recall')\n",
    "    plt.axvline(best_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Precision and Recall vs. Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy vs. Threshold Curve\n",
    "    accuracies = [(y_test == (y_proba >= t).astype(int)).mean() for t in thresholds]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(thresholds, accuracies, label='Accuracy')\n",
    "    plt.axvline(best_threshold, color='red', linestyle='--', label='Chosen Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred, y_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model at chosen threshold\n",
    "y_pred, y_proba = evaluate_model(best_rf, X_test, y_test, best_threshold, precisions, recalls, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This dataset was very interesting to model on, and I know I will do further experimentation with it going forward to improve the model performance and find new insights. After building initially, I fined tuned the results a few times to try and improve my metrics. At first, my recall was very high, but precision was quite low. That is why I built the find best threshold function to try and improve precision while keeping recall above a 0.7. When I was more satisfied with my main metrics (accuracy, precision, recall, ROC-AUC score), I thought it would be interesting to see the top ten specific features that impacted the predicitve capability of this model. From a numbers perspective, it is no suprise to me that the two most important factors were based on the action I took to start a song versus unique features like artist, title, etc... I often listen to music by letting playlists of my favorite artists play, or clicking into a specific song I want to hear. However, sometimes I let random Spotify playlists play that aren't as niche, which often results in a lot more songs not being completed by me. Looking at more unique features in the top 10, I noticed my most listened to artist, Knox, had a relatively high influence over any other artist. Also funny enough, Taylor Swift was on the list as a strong influence too, but for the opposite reason as Knox. Since this is my own streaming tendencies, I know that I have a strong likelihood to complete a song by Knox, and am likely to skip a song by Taylor Swift. I was able to confirm that with a quick true or false filter check on the CSV file itself, but would be an intersting plot to show using Pandas in the future. Overall, this was a very interesting experiment that could be further developed upon in the future for greater insights, or to even potentially influence my own personal musical recommendation system. I would love to combine this information eventually with other features available through Spotify's API such as song tempo, genre, BPM, etc... to get an even deeper understanding of what influences my likelihood to skip a song or not. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
